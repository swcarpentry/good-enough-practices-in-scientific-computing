\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{cite}

\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm 
\textheight 21cm

\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

\bibliographystyle{plos2009}
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

\date{}

\pagestyle{myheadings}

\newcommand{\fixme}[2]{\textsc{\textbf{{#1}: {#2}}}}
\newcommand{\recommend}[1]{\textit{#1}}
\newcommand{\withurl}[2]{{#1}\footnote{\texttt{#2}}}

\begin{document}

\begin{flushleft}
{\Large
\textbf{Good Enough Practices for Scientific Computing}
}

{Greg~Wilson}$^{1,\ast}$,
{Jenny~Bryan}$^{2}$,
{Karen~Cranston}$^{3}$,
{Justin~Kitzes}$^{4}$,
{Lex~Nederbragt}$^{5}$,
{Tracy~K.~Teal}$^{6}$
\\
1) Software Carpentry Foundation / gvwilson@software-carpentry.org
\\
2) University of British Columbia / jenny@stat.ubc.ca
\\
3) Duke University / karen.cranston@duke.edu
\\
4) University of California, Berkeley / jkitzes@berkeley.edu
\\
5) University of Oslo / lex.nederbragt@ibv.uio.no
\\
6) Data Carpentry / tkteal@datacarpentry.org
\\
$\ast$ E-mail: Corresponding gvwilson@software-carpentry.org
\end{flushleft}

\section{Introduction}\label{sec:introduction}

Two years ago a group of researchers involved in \withurl{Software
  Carpentry}{http://software-carpentry.org/} and \withurl{Data
  Carpentry}{http://datacarpentry.org/} wrote a paper called
``\withurl{Best Practices for Scientific
  Computing}{http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745}''
\cite{wilson2014}. It was well received, but many beginners found its
catalog of tools and techniques intimidating, and, by definition, the
``best'' are a minority, so ``best practices'' are what only a
minority does.

This paper therefore presents a set of ``good enough''
practices\footnote{Note that English lacks a good word for this:
  ``mediocre'', ``adequate'', and ``sufficient'' aren't exactly
  right.} for scientific computing, i.e., a minimum set of tools and
techniques that we believe every researcher can and should adopt. It
draws inspiration from many sources
\cite{gentzkow2014,noble2009,brown2015,wickham2014,kitzes2016,sandve2013,hart2015},
from our personal experiences, and from the experiences of the
thousands of people who have taken part in \withurl{Software
  Carpentry}{http://software-carpentry.org} and \withurl{Data
  Carpentry}{http://datacarpentry.org} workshops over the past six
years.

Our intended audience is researchers who are working alone or with a
handful of collaborators on projects lasting a few days to a few
months, and who are ready to move beyond saving spreadsheets with
names like \texttt{results-updated-3-revised.xlsx} in Dropbox. A
practice is included in our list if large numbers of researchers use
it, and large numbers of beginners are \emph{still} using it months
after first trying it out. We include the second criterion because
there is no point recommending something that people won't actually
adopt.

Many of our recommendations are for the benefit of the collaborator
every researcher cares about most: their future self\footnote{As the
  joke goes, yourself from three months ago doesn't answer
  email{\ldots}}. If researchers don't see those benefits quickly
enough to pay for the slowdown that inevitably occurs when adopting
something new, they will almost certainly switch back to their old way
of doing things.  This rules out many practices such as code review
that we feel are essential for larger-scale development
(Section~\ref{sec:omitted}).

\fixme{KC}{Add pointers to papers or projects that follow these rules.}

\subsection*{Acknowledgments}

Our thanks to Arjun Raj (University of Pennsylvania) and Steven
Haddock (Monterey Bay Aquarium Research Institute) for their feedback
on early versions of this paper, and to everyone involved in Data
Carpentry and Software Carpentry for everything they have taught us.

\section{Data Management}\label{sec:data}

Project data may need to exist in various forms, ranging from what
first arrives to what's included in the final publication.  The aim of
our recommendations is to create ``tidy data'', which can be a
powerful accelerator for analysis \cite{wickham2014,hart2015}.

\begin{enumerate}

\item
  \recommend{Save data in the rawest form available}.  Where possible,
  save the data file produced by an instrument or raw results from a
  survey, with all of its mystifying imperfections (e.g., raw JPEGs
  for photographs). It is tempting to overwrite raw data files with
  cleaned-up versions, but faithful retention is essential to being
  able to re-run analyses from start to finish. It also helps you
  recover from analytical mishaps and allows you to experiment without
  fear.

  But use common sense: if a large volume of data is received in a
  format that is inefficient to store or process, transform it for
  storage with a lossless, well-documented procedure.  Equally, don't
  duplicate contents of stable, long-lived repositories (i.e., don't
  clone GenBank).

  You may also consider restricting file permissions to read-only,
  even (or especially) for yourself, so it is harder to damage raw
  data by accident or to hand edit it in a moment of weakness.

\item
  \recommend{Create the data you wish to see in the world}, i.e.,
  create the dataset you \emph{wish} you had received from the raw
  data and use that as the starting point for downstream analyses.
  This is the time to improve machine and human readability, but
  \emph{not} the time to do vigorous data filtering or bring in
  external information.

  \fixme{JB}{Include an example.}

  Approach this initial tidying up as internal to the existing dataset
  and non-destructive, both of the data and its general ``shape''.
  Where possible, use open non-proprietary formats instead of closed,
  proprietary formats: they will probably last longer, and will
  probably be easier for machines to read.  CSV is good for simple
  tabular data, JSON, YAML, or XML for non-tabular data such as
  graphs\footnote{The node-and-arc kind.}, and HDF5 for certain kinds
  of structured data.  \withurl{This
    guide}{http://www.library.illinois.edu/sc/services/data\_management/file\_formats.html}
  has a few useful guidelines for choosing a file format.

  \fixme{TT}{Include example of what goes wrong if you don't do tidying right.}

  Enhance human readability by replacing inscrutable variable names
  and artificial data codes with self-explaining alternatives, e.g.,
  rename variables called \texttt{name1} and \texttt{name2} to
  \texttt{personal\_name} and \texttt{family\_name}, recode the
  treatment variable from \texttt{1} vs.  \texttt{2} to
  \texttt{untreated} vs. \texttt{treated}, and replace artificial
  codes for missing data, such as ``-99'', with \texttt{NA}s,
  a shorthand code for ``Not Available'' that is used as a special variable 
  in many programming languages \cite{white2013}.

  Both human and machine readability can be enhanced by storing
  especially useful metadata as part of the filename itself, while
  keeping the filename regular enough for easy pattern matching. For
  example, a filename like \texttt{2016-05-alaska-b.csv} makes it easy
  for both people and programs to select by year
  (\texttt{2016-*.csv}), by location (\texttt{*-alaska-*.csv}), and so
  on.

\item
  \recommend{Create analysis-friendly data}.  You can do this by
  reformatting data to eliminate processing steps between loading the
  data set and doing the analysis.  Columns that contain more than one
  variable's worth of information should be split. For example, the
  presence of ``kg'' in ``3.4 kg'' will cause most analytical
  environments to read this in as character data rather than
  numeric. It should be split into two columns (the mass ``3.4'' and
  the units ``kg''), or the units should be recorded in the variable
  name and/or metadata.

  Multiple columns that only contain one variable's worth of
  information when taken together should be combined. This is
  characteristic of data that has been laid out for human eyeballs or
  for manual data entry.  For example, there might be one row per
  field site and then columns for measurements made at each of several
  time points. It is convenient to store this in a ``short and wide''
  form for data entry and inspection, but for most analyses it will be
  advantageous to gather these columns into a variable of
  measurements, accompanied by a companion variable indicating the
  time point.

  Reformat values as needed to match your environment's built-in
  parsing rules. For example, use a date-time format that will be
  recognized automatically, or better yet, split dates into three
  columns (year, month, and day) for easier sorting and filtering.

  \begin{framed}
    \noindent \textbf{OpenRefine}

    \withurl{OpenRefine}{http://openrefine.org/} is an excellent tool
    for this stage of data cleanup. It combines a spreadsheet-like
    interface to tabular data with a large set of cleanup heuristics,
    and can generate a trace of cleanup steps to ensure
    reproducibility.  The cleanup process does not affect the original
    raw data, which further aids reproducibility.
  \end{framed}

\item
  \recommend{Always record the steps used to process data, 
  complete with parameter values}: it's as
  much a part of your analysis as any statistics you might do.
  All information needed to redo the steps taken to process
  the raw data should be recorded, to the extent possible. The
  best way to do this is to write a re-runnable script for
  \emph{every} stage of data processing, however trivial it seems.  
  This not only ensures that you have a record, but also allows you to
  re-do the cleanup and other steps when new data arrives. Where the
  exact details of the procedure cannot easily be written down,
  intermediate results should be saved for later checking. 
  In cases where automated data processing is not possible 
  or not practical, it's important to
  clearly document every manual action taken (what menu was used, what
  column was copied and pasted, what link was clicked, etc.) at this
  stage. For example, choosing a region of intest in an image 
  is inherently interactive, but the region chosen can be stored 
  as a set of boundary coordinates. If you are not able to document 
  this step clearly, how will anyone else be able to repeat your analysis? 
  As important, how will you know in a month whether you did it 
  right the first time?

\item
  \recommend{Give every record a unique key}.  In a well-organized
  data set, it's easy to select a specific record to update or delete.
  Using fields in the data (such as people's names or addresses) does
  not guarantee this; using row numbers in tables does, but those
  numbers will change as records are added, removed, or moved.

  We therefore recommend giving each record a unique, persistent key
  that has no other purpose than identifying the
  record\footnote{Student numbers and similar identifiers serve this
    purpose in institutional databases.} The provision of such a key
  also makes it much easier to link data sets together.

\item
  \recommend{Include complementary data in files that meet the same
    standards}.  Raw data frequently does not fully explain
  itself. For example, if each well of a microtitre plate is used to
  study a specific gene knockout, the prepared raw data will have a
  \texttt{well} variable taking on values like ``A1'' and ``G12'', but
  the plate reader cannot possibly know what's in each well. That
  information will be absent from the primary data file, so you must
  create a supplementary file in order to look this up. Make sure to
  use the same names and codes when variables in two datasets refer to
  the same thing to simplify merging and lookup (and make them less
  error-prone).

\item
  \recommend{Create a dataset purpose-built for specific analyses and
    figures.}  This probably involves filtering rows, selecting
  relevant variables, and merging with external information. If the
  preceding steps have been done, this can often be surprisingly
  straightforward. The basic form of the data was hopefully set
  earlier, so the main changes here are likely to be data reduction
  and the amalgamation of multiple datasets.  If only minimal
  selecting and merging is needed, this step may be performed by
  software written to perform data analysis without being saved
  separately.

\item
  \recommend{Submit data to a reputable DOI-issuing repository so that
    others can access and cite it.}  Your data is as much a product of
  your research as the papers you write, and just as likely to be
  useful to others (if not more so).  Sites such as
  \withurl{Figshare}{https://figshare.com/},
  \withurl{Dryad}{http://datadryad.org/}, and
  \withurl{Zenodo}{https://zenodo.org/} allow others to find your work
  and make use of it; we discuss licensing in
  Section~\ref{sec:collaboration} below.

\end{enumerate}

Taken in order, the recommendations above will produce intermediate
data files with increasing levels of cleanliness and
task-specificity. An alternative approach to data management would be
to fold all data management tasks into the procedure, in particular
the software, that is used for data analysis, so that intermediate
data products are created ``on the fly'' and stored only in memory,
not saved as distinct files.

While the latter approach may be appropriate for projects in which
very little data cleaning or processing is needed, we recommend that
beginning computational researchers explicitly create and save these
intermediate products for two reasons.  First, saving intermediate
files makes it easy to re-run \emph{parts} of a data analysis
pipeline, which in turn makes it easier and faster to tinker with
specific data cleaning operations.  Second, dividing data cleaning
into several consecutive steps and separating it from data analysis
helps to conceptually divide a lengthy workflow into smaller
conceptual chunks, akin to decomposing programs into functions (see
\textit{Software}).  Similar to dividing software into modules and
functions, chunking a scientific computing pipeline in this way often
makes it easier to understand, share, describe, and modify.

In terms of data storage, we propose delimited plain text (such as
CSV) as a lowest common denominator for tabular data, since it is
usable across time, people, operating systems, and analytical
environments. More complex data are often stored in JSON format.

\begin{framed}
  \noindent \textbf{Choosing Metadata}

  \cite{wickes2015} provides a useful classification for metadata,
  with implications for how to represent it. She notes it is important
  to distinguish metadata about the dataset as a whole (e.g., author
  and related publications) from metadata about the content (e.g.,
  units for observations).  When considering how to store metadata,
  consider the intended audience. If it is human beings, write a
  README file. If it is machines, such as metadata harvesters and
  formal repositories, create a metadata file that they will easily be
  able to parse.
\end{framed}

\section{Software}\label{sec:software}

If you or your group are creating tens of thousands of lines of
software for use by hundreds of people you have never met, you are
doing software engineering. If you're writing a few dozen lines now
and again, and are probably going to be its only user, you may not be
doing engineering, but you can still make things easier on yourself by
adopting a few key engineering practices. What's more, adopting these
practices will make it easier for people to understand and (re)use
your code.

The core realization in these practices is that \emph{readable},
\emph{reusable}, and \emph{testable} are all side effects of writing
\emph{modular} code, i.e., of building programs out of short,
single-purpose functions with clearly-defined inputs and outputs.

\begin{enumerate}

\item  
  \recommend{Place a brief explanatory comment at the start of every
    program}, no matter how short it is. That comment should include
  at least one example of how the program is used: remember, an
  example is worth a thousand words. Where possible, the comment
  should also indicate reasonable values for parameters.
  Figure~\ref{fig:comment} presents an example of of such a comment.

\item
  \recommend{Decompose programs into functions} that are no more than
  one page long (i.e., 60 lines including spaces and comments), do not
  use global variables (constants are OK), and take no more than half
  a dozen parameters.  The key motivation here is to fit the program
  into the most limited memory of all: ours. Human short-term memory
  is famously incapable of holding more than about seven (plus or
  minus two) items at once. If we are to understand what our software
  is doing, we must break it into chunks that obey this limit, then
  create programs by composing chunks into larger chunks and so on.

\item
  \recommend{Be ruthless about eliminating duplication}. Write and
  re-use functions instead of copying and pasting source code, and use
  data structures like lists rather than creating lots of variables
  called \texttt{score1}, \texttt{score2}, \texttt{score3}, etc.

  This applies with even greater force to using libraries, code written
  by others that you use as a basis for your own code. The easiest
  code to debug and maintain is code you didn't actually write, so
  \recommend{always search for well-maintained libraries that do what
    you need} before writing new code yourself, and \recommend{test
    libraries before relying on them}.

\item
  \recommend{Give functions and variables meaningful names}, both to
  document their purpose and to make the program easier to read. As a
  rule of thumb, the greater the scope of a variable, the more
  informative its name should be: while it's acceptable to call the
  counter variable in a loop \texttt{i} or \texttt{j}, the main grid
  for your simulation should \emph{not} have a one-letter name.

  \begin{framed}
    \noindent \textbf{Tab Completion}

    Almost all modern text editors provide \emph{tab completion}, so
    that typing the first part of a variable name and then pressing
    the tab key inserts the completed name of the variable.  Employing
    this means that meaningful variable names are no harder to type
    than short ones.

  \end{framed}

\item
  \recommend{Make dependencies and requirements explicit.} This is
  usually done on a per-project rather than per-program basis, i.e.,
  by adding a file called something like \texttt{requirements.txt} to
  the root directory of the project, or by adding a ``Getting
  Started'' section to the \texttt{README} file. (More sophisticated
  users may structure the requirements in a way that installation
  management tools can use.)

\item
  \recommend{Do not comment and uncomment sections of code to control
    a program's behavior}, since this is error prone and makes it
  difficult or impossible to automate analyses. Instead, put if/else
  statements in the program to control what it does.

  \begin{framed}
    \noindent \textbf{Logging}

    One special case of this recommendation is to use a logging
    library such as Java's \texttt{log4j} for debugging messages. When
    this is done, messages are left in the code, but their activity is
    controlled by an external configuration file.
  \end{framed}

\item
  \recommend{Provide a simple example or test data set} that users
  (including yourself) can run to determine whether the program is
  working at all and whether it gives a known correct output for a
  simple known input. Such as ``build and smoke test'' is particularly
  helpful when supposedly-innocent changes are being made to the
  program, or when it has to run on several different machines, e.g.,
  the developer's laptop and the department's cluster.

\item
  \recommend{Submit code to a reputable DOI-issuing repository} upon
  submission of paper, just as you do with data. Your software is as
  much a product of your research as your papers, and should be as
  easy for people to credit.

\end{enumerate}

\begin{figure}
\begin{verbatim}
Synthesize image files for testing circularity estimation algorithm.

Usage: generate_images.py -f fuzzing -n p_flaws -o output_file -r p_radius -s rand_seed -v -w size

where:
-f fuzzing     = fuzzing range of blobs (typically 0.0-0.2)
-n p_flaws     = p(success) for geometric distribution of # flaws/sample (typically 0.5-0.8)
-o output_file = name of output file
-r p_radius    = p(success) for geometric distribution of flaw radius (typically 0.1-0.4)
-s rand_seed   = RNG seed (large integer)
-v             = verbose
-w size        = image width/height in pixels (typically 480-800)
\end{verbatim}
\caption{A Good Comment To Explain The Use Of A Program}
\label{fig:comment}
\end{figure}

\section{Collaboration}\label{sec:collaboration}

You may start working on projects by yourself or with a small group of
collaborators you already know, but you should still design it to
attract new collaborators.  As summarized in \cite{steinmacher2015},
the keys to doing this are to make it easy for people set up a local
workspace so that they \emph{can} contribute, to help them find a
simple first task so that they know \emph{what} to contribute, and to
make the contribution process clear so that they know \emph{how} to
contribute.

\begin{enumerate}

\item
  \recommend{Create an overview of your project.}  Have a short
  \texttt{README} file explaining the project's purpose in the
  project's home directory.  This file (which may be called
  \texttt{README}, \texttt{README.txt}, or something similar) should
  contain the project's title, a brief description (similar to the
  abstract of a paper), contact information that actually works, and
  an example or two of how to actually run the software.  It is often
  the first thing users of your project will look at, so make it
  explicit that you welcome contributors and point them to ways they
  can help.
  
  You should also create a \texttt{CONTRIBUTING} file that describes
  what people need to do in order to get the project going and
  contribute to it, i.e., dependencies that need to be installed,
  tests that can be run to ensure the software has been installed
  correctly, and guidelines or checklists that your project adheres
  to.

\item
  \recommend{Create a shared public ``to-do'' list}.  This can be a
  plain text file called something like \texttt{notes.txt} or
  \texttt{todo.txt}, or you can use sites such as GitHub or Bitbucket
  to create a new \emph{issue} for each to-do item. (You can even add
  labels such as ``low hanging fruit'' to point newcomers at issues
  that are good starting points.)  Whatever you choose, describe the
  items clearl so that they make sense to newcomers.

\item
  \recommend{Make the license explicit.}  Have a \texttt{LICENSE} file
  in the project's home directory that clearly states what license(s)
  apply to the project's software, data, and manuscripts. Lack of an
  explicit license does not mean there isn't one; rather, it implies
  the author is keeping all rights and others are not allowed to
  re-use or modify the material.

  We recommend Creative Commons licenses for data and text, either
  \withurl{CC-0}{https://creativecommons.org/about/cc0/} (the ``No
  Rights Reserved'' license) or
  \withurl{CC-BY}{https://creativecommons.org/licenses/by/4.0/} (the
  ``Attribution'' license, which sharing and reuse but requires people
  to give appropriate credit to the creators).  For software, we
  recommend a permissive license such as the MIT, BSD, or Apache
  license \cite{laurent2004}.

  \begin{framed}
    \noindent \textbf{What Not To Do}

    We recommend \emph{against} the ``no commercial use'' variations
    of the Creative Commons licenses because they may impede some
    forms of re-use.  For example, if a researcher in a developing
    country is being paid by her government to compile a public health
    report, and wishes to include some of your data, she will be
    unable to do so if the license says ``non-commercial''. We
    recommend permissive software licenses rather than the
    \withurl{GNU General Public
      License}{https://www.safaribooksonline.com/library/view/understanding-open-source/0596005814/ch03.html}
    (GPL) because it is easier to integrate permissively-licensed
    software into other projects. (Note that it is straightforward to
    switch from a permissive license to the GPL if you should change
    your mind later but rather more complicated to go in the other
    direction.)
  \end{framed}

\item    
  \recommend{Make the project citable} by including a
  \texttt{CITATION} file in the project's home directory that
  describes how to cite this project as a whole, and where to find,
  and how to cite, any data sets, code, figures, and other artifacts
  that have their own DOIs.  Figure~\ref{fig:citation} shows the
  \texttt{CITATION} file for the \withurl{Ecodata
    Retriever}{https://github.com/weecology/retriever}; for an example
  of a more detailed \texttt{CITATION} file, see the one for the
  \withurl{khmer}{https://github.com/dib-lab/khmer/blob/master/CITATION}
  project.

\end{enumerate}

The overview and to-do list are to help you as well as other
people---remember, your most important collaborator is yourself three
months from now. The license and citation file are there to make it
easy for other people to help you and give you credit for your work.

\begin{figure}
\begin{verbatim}
Morris, B.D. and E.P. White. 2013. The EcoData Retriever: improving
 access to existing ecological data. PLOS ONE 8:e65848.
  http://doi.org/doi:10.1371/journal.pone.0065848

@article{morris2013ecodata,
  title={The EcoData Retriever: Improving Access to Existing Ecological Data},
  author={Morris, Benjamin D and White, Ethan P},
  journal={PLOS One},
  volume={8},
  number={6},
  pages={e65848},
  year={2013},
  publisher={Public Library of Science}
  doi={10.1371/journal.pone.0065848}
}
\end{verbatim}
\caption{Example CITATION File}
\label{fig:citation}
\end{figure}

\section{Project Organization}\label{sec:project}

Organizing the files that make up a project in a logical and
consistent directory structure will help you and others keep track of
them.  Our recommendations for doing this are drawn primarily from
\cite{noble2009,gentzkow2014}.

\begin{enumerate}

\item
  \recommend{Put each project in its own directory, which is named
    after the project.}  Like deciding when a chunk of code should be
  made a function, the ultimate goal of dividing research into
  distinct projects is to help you and others best understand your
  work. Some researchers create a separate project for each manuscript
  they are working on, while others group all research on a common
  theme, data set, or algorithm into a single project.
    
  As a rule of thumb, divide work into projects based on the overlap
  in data and code files. If two research efforts share no data or
  code, they will probably be easiest to manage independently. If they
  share more than half of their data and code, they are probably best
  managed together, while if you are building tools that are used in
  several projects, the common code should probably be in a project of
  its own. Anything in between can be decided based on the set of
  people you're collaborating with.

\item
  \recommend{Put text documents associated with the projectin the
    \texttt{doc} directory.} This includes files for manuscripts,
  documentation for source code, and/or an electronic lab notebook
  recording your experiments.  Subdirectories may be created for these
  different classes of files in large projects.

\item
  \recommend{Put raw data and metadata in a \texttt{data} directory,
    and files generated during cleanup and analysis in a
    \texttt{results} directory}, where ``generated files'' includes
  intermediate results, such as cleaned data sets or simulated data,
  as well as final results such as figures and tables.

  The \texttt{src} directory may contain subdirectories if there are
  many data sets.  The \texttt{results} directory will \emph{usually}
  require additional subdirectories for all but the simplest
  projects. Intermediate files such as cleaned data, statistical
  tables, and final publication-ready figures or tables should be
  separated clearly by file naming conventions or placed into
  different subdirectories; those belonging to different papers or
  other publications should be grouped together.

\item
  \recommend{Put source for this project's scripts and programs in the
    \texttt{src} directory, and scripts and programs brought in from
    elsewhere or compiled locally in the \texttt{bin} directory.}
  \texttt{src} contains both programs written in interpreted languages
  such as R or Python and those written compiled languages like
  Fortran, C++, or Java. Shell scripts, snippets of SQL used to pull
  information from databases, and everything else needed to regenerate
  the results are all managed like source code.  \texttt{bin} contains
  scripts that are brought in from elsewhere, and executable programs
  compiled from code in the \texttt{src} directory\footnote{The name
    \texttt{bin} is an old Unix convention, and comes from the term
    ``binary''}. Projects that have neither will not require this
  directory.

  \begin{framed}
    \noindent \textbf{Scripts vs.\ Programs}

    We use the term ``script'' to mean ``something that is executed
    directly as-is'', and ``program'' to mean ``something that is
    explicitly compiled before being used''.  The distinction is more
    one of degree than kind---libraries written in Python are actually
    compiled to bytecode as they are loaded, for example---so one
    other way to think of it is ``things that are edited directly''
    and ``things that are not''.
  \end{framed}

\item
  \recommend{Name all files to reflect their content or function.} For
  example, use names such as \texttt{bird\_count\_table.csv},
  \texttt{manuscript.md}, or \texttt{sightings\_analysis.py}).  Do
  \emph{not} using sequential numbers (e.g., \texttt{result1.csv},
  \texttt{result2.csv}) or a location in a final manuscript (e.g.,
  \texttt{fig\_3\_a.png}), since those numbers will almost certainly
  change as the project evolves.
  
\end{enumerate}

The \texttt{src} directory may contain two conceptually distinct types
of files that should be distinguished either by clear file names or by
additional subdirectories. The first type are individual files or
related groups of files that contain functions to perform the core
analysis of the research. One file, for example, may contain functions
used for data cleaning, while another contains functions to do certain
statistical analyses. These files can be thought of as the
``scientific guts'' of the project, and as the project grows, they can
be organized into additional subdirectories. If a project were to
include re-runnable tests (Section~\ref{sec:omitted}), these are the
functions they would test.
  
The second type of file in \texttt{src} is controller or driver
scripts that combine the core analytical functions with particular
parameters and data input/output commands in order to execute the
entire project analysis from start to finish. A controller script for
a simple project, for example, may read a raw data table, import and
apply several cleanup and analysis functions from the other files in
this directory, and create and save a numeric result. For a small
project with one main output, a single controller script should be
placed in the main \texttt{src} directory and distinguished clearly by
a name such as ``runall''.

The controller script is the glue that holds the analysis together and
allows a single command, such as \texttt{python runall.py}, to re-run
the entire analysis from start to finish. These scripts should be
short (no more than one or two pages long) and very easy to
understand.  In particular, while they may contain loops (to process
multiple data files or sweep across parameter ranges) they should
contain few if any conditional statements or new function
definitions. If a control script becomes longer or more complicated
than this, or begins to include code that would require a new
collaborator more than a minute or two to understand, those portions
of the code should be moved out of the controller script and into
other core analysis files in this directory.

Figure~\ref{fig:layout} below provides a concrete example of how a
simple project might be organized following these recommendations. The
root directory contains a \texttt{README} file that provides an
overview of the project as a whole and a \texttt{CITATION} file that
explains how to reference it. The \texttt{data} directory contains a
single CSV file with tabular data on bird counts (machine-readable
metadata could also be included here). The \texttt{src} directory
contains \texttt{sightings\_analysis.py}, a Python file containing
functions to summarize the tabular data, and a controller script
\texttt{runall.py} that loads the data table, applies functions
imported from \texttt{sightings\_analysis.py}, and saves a table of
summarized results in the \texttt{results} directory.

This project doesn't have a \texttt{bin} directory, since it does not
rely on any compiled software. The \texttt{doc} directory contains two
text files written in Markdown, one containing a running lab notebook
describing various ideas for the project and how these were
implemented and the other containing a running draft of a manuscript
describing the project findings.

\begin{figure}
\begin{verbatim}
.
|-- CITATION
|-- README
|-- data
|   -- birds_count_table.csv
|-- doc
|   -- notebook.md
|   -- manuscript.md
|-- results
|   -- summarized_results.csv
|-- src
|   -- sightings_analysis.py
|   -- runall.py
\end{verbatim}
\caption{Example Project Layout}
\label{fig:layout}
\end{figure}

\section{Version Control}\label{sec:versioning}

Keeping track of changes that you or your collaborators make to data
and software is a critical part of research.  We believe that the best
tools for doing this are the version control systems that are used in
software development, such as Git, Mercurial, and Subversion.  They
keep track of what was changed in a file when and by whom, and
synchronize changes to a central server so that many users can track
the same set of files.

Although all of the authors use version control daily for all of their
projects, we recognize that many beginning computational scientists
find version control to be one of the more difficult practices to
adopt.  We therefore recommend that projects adopt \emph{either} a
systematic manual approach for managing changes \emph{or} version
control in its full glory.  Whatever is adopted should aid
reproducibility by allowing you to reference or retrieve a specific
version of the entire project. This is valuable for your future self
(when you finally get the reviews back for your paper), for your
lab-mates and collaborators (in case you leave the project), and for
reviewers, editors, and others who want to convince themselves of the
conclusions in your published research.  It should also support
sharing and collaboration by managing the process of merging
independent changes made by different people, and distributing those
changes back to everyone in a controlled, traceable way.

Whatever is chosen, we recommend that it be used in the following way:

\begin{enumerate}

\item
  \recommend{Back up (almost) everything created by a human being as
    soon as it is created.} This includes scripts and programs of all
  kinds, software packages that your project depends on, and
  documentation. A few exceptions to this rule are discussed below.

\item
  \recommend{Keep changes small.}  Each change should not be so large
  as to make the change tracking irrelevant. For example, a single
  change such as ``Revise script file'' that adds or changes several
  hundred lines is likely too large, as it will not allow changes to
  different components of an analysis to be investigated
  separately. Similarly, changes should not be broken up into pieces
  that are too small, although we find that this is less of a danger
  with beginners. As a rule of thumb, a good size for a single change is
  a group of edits that you could imagine wanting to undo in one step
  at some point in the future.

\item
  \recommend{Share changes frequently.} Everyone working on the
  project should share and incorporate changes from others on a
  regular basis.  Do not allow individual investigator's versions of
  the project repository to drift apart, as the effort required to
  merge differences goes up faster than the size of the
  difference. This is particularly important for the manual version
  control procedure describe above, which does not provide any
  assistance for merging simultaneous changes.

\item
  \recommend{Create, maintain, and use a checklist for committing and
    sharing changes to the project.}  The list should include writing
  commit messages that clearly explain any changes, the size and
  content of single commits, style guidelines for code, updating to-do
  lists, and bans on committing half-done work or broken code.  See
  \cite{gawande2011} for more on the proven value of checklists.

\end{enumerate}

\subsection*{Manual Versioning}

Our first suggested approach, in which everything is done by hand, has
three parts:

\begin{enumerate}

\item
  \recommend{Store each project in a folder that is mirrored off the
    researcher's working machine} by a system such as Dropbox, and
  synchronize that folder at least daily. It may take a few minutes,
  but that time can be spent catching up on email, and is repaid the
  moment a laptop is stolen or its hard drive fails.

\item
  \recommend{Add a file called \texttt{CHANGELOG.txt} to the project's
    \texttt{docs} subfolder}, and make dated notes about changes to
  the project in this file in reverse chronological order (i.e., most
  recent first). This file is the equivalent of a lab notebook, and
  should contain entries like those shown in
  Figure~\ref{fig:changelog}.

\item
  \recommend{Copy the entire project whenever a significant change has
    been made} (i.e., one that materially affects the results being
  produced), and store that copy in a sub-folder whose name reflects
  the date in the area that's being synchronized. This approach
  results in projects being organized as shown in
  Figure~\ref{fig:manual}.  Here, the \texttt{project\_name} folder is
  mapped to external storage (such as Dropbox), \texttt{current} is
  where development is done, and other folders within
  \texttt{project\_name} are old versions.

  \begin{framed}
    \noindent \textbf{Data is Cheap, Time is Expensive}

    Copying everything like this may seem wasteful, since many files
    won't have changed, but consider: a terabyte hard drive costs
    about \$50 retail, which means that 50 GByte costs less than a
    latte. Provided large data files are kept out of the backed-up
    area (discussed below), this approach costs less than the time it
    would take to select files by hand for copying.
  \end{framed}

\end{enumerate}

\begin{figure}
\begin{verbatim}
## 2016-04-08

* Switched to cubic interpolation as default.
* Moved question about family's TB history to end of questionnaire.

## 2016-04-06

* Added option for cubic interpolation.
* Removed question about staph exposure (can be inferred from blood test results).
\end{verbatim}
\caption{Example Changelog}
\label{fig:changelog}
\end{figure}

\begin{figure}
\begin{verbatim}
.
|-- project_name
|   -- current
|       -- ...project content as described earlier...
|   -- 2016-03-01
|       -- ...content of 'current' on Mar 1, 2016
|   -- 2016-02-19
|       -- ...content of 'current' on Feb 19, 2016
\end{verbatim}
\caption{Directory Hierarchy for Manual Versioning}
\label{fig:manual}
\end{figure}

This manual procedure satisfies the requirements outlined above
without needing any new tools. If multiple researchers are working on
the same project, though, they will need to coordinate so that only a
single person is working on specific files at any time. In particular,
they may wish to create one change log file per contributor, and to
merge those files whenever a backup copy is made.

\subsection*{Tool-Based Versioning}

What the manual process described above requires most is
self-discipline. The version control tools that underpin our second
approach---the one we all now use for our projects---don't just
accelerate the manual process: they also automate some steps while
enforcing others, and thereby require less self-discipline for more
reliable results.

\begin{framed}
  \noindent \textbf{How Version Control Works}

  A version control system stores snapshots of a project's files in a
  repository. Users can modify their working copy of the project at
  will, and then commit changes to the repository when they wish to
  make a permanent record and/or share their work with colleagues. The
  version control system automatically records when the change was
  made and by whom along with the changes themselves.
  
  Crucially, if several people have edited files simultaneously, the
  version control system will detect the collision and require them to
  resolve any conflicts before recording the changes. Modern version
  control systems also allow repositories to be synchronized with each
  other, so that no one repository becomes a single point of failure.
  Tool-based version control has several benefits over manual version
  control:

  \begin{itemize}

  \item
    Instead of requiring users to copy everything into subfolders,
    version control safely stores just enough information to allow old
    versions of files to be re-created on demand. This saves both
    space and time.

  \item
    Instead of relying on users to choose sensible names for backup
    copies, the version control system timestamps all saved changes
    automatically.

  \item
    Instead of requiring users to be disciplined about writing log
    comments, version control systems prompt them every time a change
    is saved. They also keep a 100\% accurate record of what was
    \emph{actually} changed, as opposed to what the user
    \emph{thought} they changed, which can be invaluable when problems
    crop up later.

  \item
    Instead of simply copying files to remote storage, version control
    checks to see whether doing that would overwrite anyone else's
    work.  This turns out to be the key to supporting large-scale ad
    hoc collaboration.

  \end{itemize}
\end{framed}

It's hard to know what version control tool is most widely used in
research today, but the one that's most talked about is undoubtedly
\withurl{Git}{https://git-scm.com/}.  This is largely because of the
popularity of \withurl{GitHub}{http://github.com}, a hosting site that
provides free repositories to those willing to make their work openly
accessible.  For those who find Git's command-line syntax inconsistent
and confusing, \withurl{Mercurial}{https://www.mercurial-scm.org/} is
a good choice; \withurl{Bitbucket}{https://bitbucket.org/} provides
free hosting for both Git and Mercurial repositories, but does not
have nearly as many scientific users.

\subsection*{When is Version Control Not Necessary?}

Despite the benefits of version control systems, some types of files
may \emph{not} be appropriate for version control. While placing small
files in a version control repository facilitates reproducibility,
today's version control systems are not designed to handle
megabyte-sized files, never mind gigabytes, although support for them
is emerging.

Second, raw data should not change, and therefore should not require
version tracking. Putting synthesized or modified data sets into
version control may not be necessary if you can re-generate these
files from raw data and data-cleaning scripts (which definitely
\emph{are} under version control!).

Third, some data formats are unfortunately not amenable to version
control, which is designed to work with plain text files such as
source code.  In particular, Microsoft Office files (like the
\texttt{.docx} files used by Word or the \texttt{.xlsx} files used by
Excel) can be stored in a version control system, but you won't be
able to see specific changes. Similarly, tabular data (such as CSV
files) can be put in version control, but changing the order of the
rows or columns will create a big change for the version control
system, even if the data itself has not changed.

\begin{framed}
  \noindent \textbf{Inadvertent Sharing}

  Researchers dealing with data subject to legal restrictions that
  prohibit sharing (such as medical data) should be careful not to put
  data in public version control systems. Some institutions may
  provide access to private version control systems, so it is worth
  checking with your IT department.
\end{framed}

Opinion is divided on whether the \texttt{results} directory and other
generated files such as figures should be placed under version
control. If we borrow conventions from software development (just as
we borrowed version control itself) the answer is no, but there are
some benefits to putting results under version control in data
analysis projects:

\begin{itemize}

\item
  It gives collaborators immediate access to current processed data,
  results, figures, etc., without needing to regenerate it all.

\item
  Version control facilitates \emph{diffing}, i.e., seeing the
  differences between old and new states of files. Diffs can be used
  to see the downstream effects of actions like upgrading a piece of
  software, refactoring a script, or starting with a slightly
  different dataset.

\end{itemize}

If results files are kilobytes or a few megabytes in size, we
therefore recommend keeping them under version control. Anything more
than this, and something else should be used for management.

\section{Manuscripts}\label{sec:manuscripts}

Gathering data, analyzing it, and figuring out what it means is the
first 90\% of any project; writing up is the other 90\%. While writing
is rarely addressed in discussions of scientific computing, computing
has changed it just as much as it has changed research.

A common practice in academic writing is for the lead author to send
successive versions of a manuscript to coauthors to collect feedback,
which is returned as changes to the document, comments on the
document, plain text in email, or a mix of all three. This results in
a lot of files to keep track of, and a lot of tedious manual labor to
merge comments to create the next master version.

Instead of an email-based workflow, we recommend mirroring good
practices for managing software and data to make writing scalable,
collaborative, and reproducible.  As with our recommendations for
version control in general, we suggest that groups choose one of two
different approaches for managing manuscripts.  The goals of both are
to:

\begin{itemize}

\item
  Ensure that text is accessible to yourself and others now and in the
  future by making a single master document that is available to all
  coauthors at all times.

\item
  Reduce the chances of work being lost or people overwriting each
  other's work.

\item
  Make it easy to track and combine contributions from multiple
  collaborators.

\item
  Avoid duplication and manual entry of information, particularly in
  constructing bibliographies, tables of contents, and lists.

\item
  Make it easy to regenerate the final published form (e.g., a PDF)
  and to tell if it is up to date.

\item
  Make it easy to share that final version with collaborators and to
  submit it to a journal.

\end{itemize}

\begin{framed}
  \noindent \textbf{The First Rule Is{\ldots}}

  Which workflow is chosen is less important than having all authors
  agree on one or the other \emph{before} writing starts. Make sure to
  also agree on a single method to provide feedback, be it an email
  thread or mailing list, an issue tracker (like the ones provided by
  GitHub and Bitbucket), or some sort of shared online to-do list.
\end{framed}

\subsection*{Single Master Online}

Our first alternative has two parts:

\begin{enumerate}

\item
  \recommend{Write manuscripts using online tools with rich
    formatting, change tracking, and reference management}, such as
  Google Docs.

\item
  \recommend{Include a \texttt{PUBLICATIONS} file in the project's
    \texttt{doc} directory} with metadata about each online manuscript
  (e.g., their URLs). This is analogous to the \texttt{data}
  directory, which might contain links to the location of the data
  file(s) rather than the actual files.

\end{enumerate}

We realize that in many cases, even this solution is asking too much
from those who see no reason to move forward from desktop GUI
tools. To satisfy them, the manuscript can be converted to a desktop
editor file format (e.g., Microsoft Word's \texttt{.docx} or
LibreOffice's \texttt{.odt}) after major changes, then downloaded and
saved in the \texttt{doc} folder. Unfortunately, this means merging
some changes and suggestions manually, as existing tools cannot always
do this automatically when switching from a desktop file format to
text and back (although \withurl{Pandoc}{http://pandoc.org/} can go a
long way).

\subsection*{Version Control}

The second approach treats papers exactly like software, and has been
used by researchers in mathematics, astronomy, physics, and related
disciplines for decades:

\begin{enumerate}

\item
  \recommend{Write the manuscript in a plain text format that permits
    version control} such as
  \withurl{LaTeX}{http://www.latex-project.org/} or
  \withurl{Markdown}{http://daringfireball.net/projects/markdown/},
  and then convert them to other formats such as PDF as needed using
  scriptable tools like \withurl{Pandoc}{http://pandoc.org/}.

\item
  \recommend{Include tools needed to compile manuscripts in the
    project folder} and keep them under version control just like
  tools used to do simulation or analysis.

\end{enumerate}

This approach re-uses the version control tools and skills used to
manage data and software, and is a good starting point for
fully-reproducible research. However, it requires all contributors to
understand a much larger set of tools, including markdown or LaTeX,
make, BiBTeX, and Git/GitHub.

\subsection*{Why Two?}

The first draft of this paper recommended always using plain text in
version control to manage manuscripts, but several reviewers pushed
back forcefully. For example, Stephen Turner wrote:

\begin{quote}
{\ldots}try to explain the notion of compiling a document to an
overworked physician you collaborate with. Oh, but before that, you
have to explain the difference between plain text and word
processing. And text editors. And markdown/LaTeX compilers. And
BiBTeX. And Git. And GitHub. Etc. Meanwhile he/she is getting paged
from the OR{\ldots}

{\ldots}as much as we want to convince ourselves otherwise, when you
have to collaborate with those outside the scientific computing
bubble, the barrier to collaborating on papers in this framework is
simply too high to overcome. Good intentions aside, it always comes
down to ``just give me a Word document with tracked changes,'' or
similar.
\end{quote}

Similarly, Arjun Raj said in \withurl{a blog
  post}{http://rajlaboratory.blogspot.ca/2016/03/from-over-reproducibility-to.html}:

\begin{quote}
Google Docs excels at easy sharing, collaboration, simultaneous
editing, commenting and reply-to-commenting. Sure, one can approximate
these using text-based systems and version control. The question is
why anyone would like to{\ldots}

The goal of reproducible research is to make sure one
can{\dots}reproduce{\ldots}computational analyses. The goal of version
control is to track changes to source code. These are fundamentally
distinct goals, and while there is some overlap, version control is
merely a tool to help achieve that, and comes with so much overhead
and baggage that it is often not worth the effort.
\end{quote}

In keeping with our goal of recommending ``good enough'' practices, we
have therefore included online storage in something like Google
Docs. We still recommend \emph{against} traditional desktop tools like
LibreOffice and Microsoft Word because they make collaboration more
difficult than necessary:

\begin{itemize}

\item
  If the document lives online (e.g., in Google Docs) then everyone's
  changes are in one place, and hence don't need to be merged
  manually.

\item
  If the document lives in a version control system, it provides good
  support for finding and merging differences resulting from
  concurrent changes. It also provides a convenient platform for
  making comments and performing review.

\item
  Both of our recommendations clearly define the master document and
  allow everyone to contribute to it on an equal footing.

\end{itemize}

\begin{framed}
  \noindent \textbf{Linking Instead of Embedding}

  One way to make desktop writing tools less error-prone is to insert
  a link to an image file rather than embedding the file directly.
  This makes the document slightly more difficult to share, since the
  image must be sent along with the document, but the payoff is that
  when the image is updated, the document automatically reflects those
  changes.  In particular, when a script is run to generate a new
  version of a figure, that figure automatically shows up in the right
  place in the paper.

  This trick does not work for tables: there is, for example, no way
  to link to a CSV file.  However, most scientific programming tools
  are able to generate tables as HTML; if this is viewed in the
  browser, it can then be copied and pasted into a document.  Doing
  this is not ideal, but is still better than copying and pasting
  individual values.
\end{framed}

\subsection*{Supplementary Materials}

Supplementary materials often contain much of the work that went into
the project, such as tables and figures or more elaborate descriptions
of the algorithms, software, methods, and analyses. In order to make
these materials as accessible to others as possible, do not rely
solely on the PDF format, since extracting data from PDFs is
notoriously hard.  Instead, we recommend separating the results that
you may expect others to reuse (e.g., data in tables, data behind
figures) into separate, text-format files. The same holds for any
commands or code you want to include as supplementary material: use
the format that most easily enables reuse (source code files, Unix
shell scripts etc).

\section{What We Left Out}\label{sec:omitted}

We have deliberately left many good tools and practices off our list,
including some that we use daily, because they only make sense on top
of the core practices described above, or because it takes a larger
investment before they start to pay off.

\begin{description}

\item[\textbf{Branches}] A \emph{branch} is a ``parallel universe''
  within a version control repository. Developers create branches so
  that they can make multiple changes to a project independently. They
  are central to the way that experienced developers use systems like
  Git, but they add an extra layer of complexity to version control
  for newcomers.  Programmers got along fine in the days of CVS and
  Subversion without relying heavily on branching, and branching can
  be adopted without significant disruption after people have mastered
  a basic edit-commit workflow.

\item[\textbf{Build Tools}] Tools like
  \withurl{Make}{https://www.gnu.org/software/make/} were originally
  developed to recompile pieces of software that had fallen out of
  date. They are now used to regenerate data and entire papers: when
  one or more raw input files change, Make can automatically re-run
  those parts of the analysis that are affected, regenerate tables and
  plots, and then regenerate the human-readable PDF that depends on
  them.  However, beginenrs can achieve the same behavior by writing
  shell scripts that re-run everything; these may do unnecessary work,
  but given the speed of today's machines, that is unimportant for
  small projects.

\item[\textbf{Unit Tests}] A \emph{unit test} is a small test of one
  particular feature of a piece of software. Projects rely on unit
  tests to prevent \emph{regression}, i.e., to ensure that a change to
  one part of the software doesn't break other parts. While unit tests
  are essential to the health of large libraries and programs, we have
  found that they usually aren't compelling for solo exploratory
  work. (Note, for example, the lack of a \texttt{test} directory in
  Noble's rules \cite{noble2009}.)  Rather than advocating something
  which people are unlikely to adopt, we have left unit testing off
  this list.

\item[\textbf{Continuous Integration}] Tools like
  \withurl{Travis-CI}{https://travis-ci.org/} automatically run a set
  of user-defined commands whenever changes are made to a version
  control repository. These commands typically execute tests to make
  sure that software hasn't regressed, i.e., that things which used to
  work still do. These tests can be run either before the commit takes
  place (in which case the changes can be rejected if something fails)
  or after (in which case the project's contributors can be notified
  of the breakage). CI systems are invaluable in large projects with
  many contributors, but pay fewer dividends in smaller projects where
  code is being written to do specific analyses.

\item[\textbf{Profiling and Performance Tuning}] \emph{Profiling} is
  the act of measuring where a program spends its time, and is an
  essential first step in \emph{tuning} the program (i.e., making it
  run faster). Both are worth doing, but only when the program's
  performance is actually a bottleneck: in our experience, most users
  spend more time getting the program right in the first place.

\item[\textbf{Coverage}] Every modern programming language comes with
  tools to report the \emph{coverage} of a set of test cases, i.e.,
  the set of lines that are and aren't actually executed when those
  tests are run. Mature projects run these tools periodically to find
  code that isn't being used any more, but as with unit testing, this
  only starts to pay off once the project grows larger, and is
  therefore not recommended here.

\item[\textbf{The Semantic Web}] Ontologies and other formal
  definitions of data are useful, but in our experience, even
  simplified things like \withurl{Dublin Core}{http://dublincore.org/}
  are rarely encountered in the wild.

\item[\textbf{Documentation}] Good documentation is a key factor in
  software adoption, but in practice, people won't write comprehensive
  documentation until they have collaborators who will use it. They
  will, however, quickly see the point of a brief explanatory comment
  at the start of each script, so we have recommended that as a first
  step.

\item[\textbf{A Bibliography Manager}] Researchers should use a
  reference manager of some sort, such as
  \withurl{Zotero}{http://zotero.org/}, and should also obtain and use
  an \withurl{ORCID}{http://orcid.org/} to identify themselves in
  their publications, but discussion of those is outside the scope of
  this paper.

\item[\textbf{Code Reviews and Pair Programming}] These practices are
  valuable in projects with multiple contributors, but are hard to
  adopt in single-author/single-user situations, which includes most
  of the intended audience for this paper \cite{petre2014}.

\end{description}

One important observation about this list is that many experienced
programmers actually do some or all of these things even for small
projects. It makes sense for them to do so because (a) they've already
paid the learning cost of the tool, so the time required to implement
for the ``next'' project is small, and (b) they understand that their
project will need some or all of these things as it scales, so they
might as well put it in place now.

The problem comes when those experienced developers give advice to
beginners who \emph{haven't} already mastered the tools, and
\emph{don't} realize (yet) that they will save time if and when their
project grows.  In that situation, advocating unit testing with
coverage checking and continuous integration is as likely to scare
beginners off than to aid them.

\section{Conclusion}\label{sec:conclusion}

\fixme{GW}{write a conclusion}

\bibliography{good-enough-practices-for-scientific-computing}

\end{document}
